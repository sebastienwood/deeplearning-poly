{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# based on http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from collections import namedtuple\n",
    "# from my_model import myAgent\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.fc3(x)\n",
    "        return q\n",
    "\n",
    "\n",
    "# from https://github.com/ghliu/pytorch-ddpg/blob/master/util.py\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, gamma=0.99, batch_size=128):\n",
    "        self.target_Q = DQN()\n",
    "        self.Q = DQN()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 128\n",
    "        hard_update(self.target_Q, self.Q)\n",
    "        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=0.001)\n",
    "\n",
    "    def act(self, x, epsilon=0.1):\n",
    "        # TODO\n",
    "        # fonction utiles: torch.max()\n",
    "        # select the maximizing a\n",
    "        # return an integer\n",
    "        if random.random() > epsilon:\n",
    "            #import  pdb; pdb.set_trace()\n",
    "            _, act = self.Q(x).view(1,2).max(1)\n",
    "            return act\n",
    "        # explore\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "            act = Variable(torch.LongTensor([act]))\n",
    "            return act\n",
    "\n",
    "\n",
    "    def backward(self, transitions):\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # TODO\n",
    "        # fonctions utiles: torch.gather(), torch.detach()\n",
    "        # torch.nn.functional.smooth_l1_loss()\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "        non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None]), volatile=True)\n",
    "\n",
    "        state_batch = Variable(torch.cat(batch.state))\n",
    "        action_batch = Variable(torch.cat(batch.action))\n",
    "        reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "        actdeep = self.Q(state_batch)\n",
    "        state_action_values = actdeep.gather(1, action_batch.view(-1, 1))\n",
    "\n",
    "        next_state_values = Variable(torch.zeros(self.batch_size).type(torch.FloatTensor))\n",
    "        next_state_values[non_final_mask] = self.Q(non_final_next_states).max(1)[0]\n",
    "        next_state_values.volatile = False\n",
    "\n",
    "        # Huber Loss computation\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.Q.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = Agent()\n",
    "memory = ReplayMemory(100000)\n",
    "batch_size = 128\n",
    "\n",
    "epsilon = 1\n",
    "rewards = []\n",
    "\n",
    "for i in range(5000):\n",
    "    obs = env.reset()\n",
    "    # env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    epsilon *= 0.99\n",
    "    while not done:\n",
    "        epsilon = max(epsilon, 0.1)\n",
    "        obs_input = Variable(torch.from_numpy(obs).type(torch.FloatTensor))\n",
    "        action = agent.act(obs_input, epsilon)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(int(action.data.numpy()[0]))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        memory.push(obs_input.data.view(1,-1), action.data,\n",
    "                    torch.from_numpy(next_obs).type(torch.FloatTensor).view(1,-1), torch.Tensor([reward]),\n",
    "                   torch.Tensor([done]))\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    rewards.append(total_reward)\n",
    "    if memory.__len__() > 10000:\n",
    "        batch = memory.sample(batch_size)\n",
    "        agent.backward(batch)\n",
    "\n",
    "pd.DataFrame(rewards).rolling(50, center=False).mean().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
